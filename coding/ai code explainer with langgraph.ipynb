{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9f5448-a40f-40c9-b881-534b843d4485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Python code snippet:  import google.generativeai as genai from langchain_google_genai import ChatGoogleGenerativeAI from langgraph.graph import StateGraph  # ‚úÖ Set API key for Gemini api_key = \"AIzaSyBNX8KAXoSP2x3JejdKEX_Dit48YVJR1xg\"  # Replace with your actual key genai.configure(api_key=api_key)  # ‚úÖ Initialize Gemini 2.0 Flash gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)  # ‚úÖ Define State Schema for LangGraph class BlogState:     def __init__(self, topic):         self.topic = topic         self.outline = None         self.sections = None         self.final_content = None  # ‚úÖ Step 1: Generate an Outline def generate_outline(state):     prompt = f\"Create a structured blog outline for the topic: {state['topic']}.\"     response = gemini.invoke(prompt).content     return {\"outline\": response}  # ‚úÖ Step 2: Expand Sections in One API Call def expand_sections(state):     outline = state[\"outline\"]     prompt = f\"Expand each section from this outline:\\n{outline}\"     response = gemini.invoke(prompt).content     return {\"sections\": response}  # ‚úÖ Step 3: Refine the Content def refine_content(state):     prompt = f\"Refine this blog content for clarity and readability:\\n{state['sections']}\"     response = gemini.invoke(prompt).content     return {\"final_content\": response}  # ‚úÖ Build LangGraph Workflow workflow = StateGraph(dict)  # Use dict instead of class-based state  workflow.add_node(\"generate_outline\", generate_outline) workflow.add_node(\"expand_sections\", expand_sections) workflow.add_node(\"refine_content\", refine_content)  workflow.set_entry_point(\"generate_outline\") workflow.add_edge(\"generate_outline\", \"expand_sections\") workflow.add_edge(\"expand_sections\", \"refine_content\")  # ‚úÖ Compile the Workflow graph = workflow.compile()  # ‚úÖ User Input and Execution user_topic = input(\"Enter a topic for the blog: \") state = {\"topic\": user_topic} output = graph.invoke(state)  # ‚úÖ Print Final Blog Content print(\"\\nüìù Final Generated Blog Post:\\n\") print(output[\"final_content\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Code Structure:\n",
      " Okay, let's break down the structure and components of this Python code.\n",
      "\n",
      "**1. Imports:**\n",
      "\n",
      "*   `google.generativeai as genai`: Imports the Google Generative AI library, aliased as `genai`. This library is likely used for configuring the API key and interacting with the Gemini models.\n",
      "*   `langchain_google_genai.ChatGoogleGenerativeAI`: Imports a class from `langchain_google_genai` that facilitates using Google Generative AI models within the Langchain framework, specifically for chat-based interactions.\n",
      "*   `langgraph.graph.StateGraph`: Imports the `StateGraph` class from the `langgraph` library.  This is the core class used to define the workflow.\n",
      "\n",
      "**2. API Key Configuration:**\n",
      "\n",
      "*   `api_key = \"AIzaSyBNX8KAXoSP2x3JejdKEX_Dit48YVJR1xg\"`:  Defines a string variable `api_key` to hold the Google Generative AI API key.  **Important:** This is a placeholder.  A real API key should be stored securely (e.g., environment variable) and not hardcoded directly in the script.\n",
      "*   `genai.configure(api_key=api_key)`: Configures the `google.generativeai` library with the provided API key.\n",
      "\n",
      "**3. Gemini Model Initialization:**\n",
      "\n",
      "*   `gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)`: Creates an instance of the `ChatGoogleGenerativeAI` class, configuring it to use the \"gemini-2.0-flash\" model and the provided API key.  This `gemini` object is used to interact with the Gemini model.\n",
      "\n",
      "**4. State Management (Data Class - Replaced with dict)**\n",
      "\n",
      "*   The original code defined a `BlogState` class, but the code was updated to use a `dict` instead.  This change makes the state management simpler. The `dict` stores the topic, outline, sections, and final content of the blog post.  This state is passed between the different steps in the LangGraph workflow.\n",
      "\n",
      "**5. Functions (Nodes in the LangGraph):**\n",
      "\n",
      "These functions define the individual steps in the blog post generation workflow.  Each function takes the current state as input and returns a dictionary containing the updated state.\n",
      "\n",
      "*   **`generate_outline(state)`:**\n",
      "    *   Takes the current `state` (a dictionary) as input.\n",
      "    *   Constructs a prompt to generate a blog outline based on the `topic` in the state.\n",
      "    *   Calls the `gemini.invoke(prompt)` method to get the outline from the Gemini model.\n",
      "    *   Returns a dictionary with the generated outline: `{\"outline\": response}`.\n",
      "\n",
      "*   **`expand_sections(state)`:**\n",
      "    *   Takes the current `state` as input.\n",
      "    *   Extracts the `outline` from the state.\n",
      "    *   Constructs a prompt to expand each section of the outline.\n",
      "    *   Calls `gemini.invoke(prompt)` to get the expanded sections from the Gemini model.\n",
      "    *   Returns a dictionary with the expanded sections: `{\"sections\": response}`.\n",
      "\n",
      "*   **`refine_content(state)`:**\n",
      "    *   Takes the current `state` as input.\n",
      "    *   Extracts the `sections` from the state.\n",
      "    *   Constructs a prompt to refine the blog content for clarity and readability.\n",
      "    *   Calls `gemini.invoke(prompt)` to get the refined content from the Gemini model.\n",
      "    *   Returns a dictionary with the refined content: `{\"final_content\": response}`.\n",
      "\n",
      "**6. LangGraph Workflow Definition:**\n",
      "\n",
      "*   `workflow = StateGraph(dict)`: Creates an instance of the `StateGraph` class, using a `dict` as the state.\n",
      "*   `workflow.add_node(\"generate_outline\", generate_outline)`: Adds the `generate_outline` function as a node in the graph, named \"generate_outline\".\n",
      "*   `workflow.add_node(\"expand_sections\", expand_sections)`: Adds the `expand_sections` function as a node.\n",
      "*   `workflow.add_node(\"refine_content\", refine_content)`: Adds the `refine_content` function as a node.\n",
      "*   `workflow.set_entry_point(\"generate_outline\")`: Sets the \"generate_outline\" node as the starting point of the workflow.\n",
      "*   `workflow.add_edge(\"generate_outline\", \"expand_sections\")`: Defines an edge (connection) from the \"generate_outline\" node to the \"expand_sections\" node, indicating the execution flow.\n",
      "*   `workflow.add_edge(\"expand_sections\", \"refine_content\")`: Defines an edge from the \"expand_sections\" node to the \"refine_content\" node.\n",
      "\n",
      "**7. Workflow Compilation:**\n",
      "\n",
      "*   `graph = workflow.compile()`: Compiles the `StateGraph` into an executable graph object (`graph`).\n",
      "\n",
      "**8. User Input and Execution:**\n",
      "\n",
      "*   `user_topic = input(\"Enter a topic for the blog: \")`: Prompts the user to enter a topic for the blog.\n",
      "*   `state = {\"topic\": user_topic}`: Creates the initial state dictionary with the user-provided topic.\n",
      "*   `output = graph.invoke(state)`: Executes the LangGraph workflow by invoking the compiled graph with the initial state. The `output` variable will contain the final state after the workflow has completed.\n",
      "\n",
      "**9. Output:**\n",
      "\n",
      "*   `print(\"\\nüìù Final Generated Blog Post:\\n\")`: Prints a header for the final blog post.\n",
      "*   `print(output[\"final_content\"])`: Prints the `final_content` from the output (which is the final state dictionary), which represents the generated blog post.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The code defines a LangGraph workflow to generate a blog post. The workflow consists of three steps: generating an outline, expanding the sections, and refining the content. Each step is implemented as a function that interacts with the Gemini model using the Langchain integration. The `StateGraph` class is used to define the workflow and the flow of data between the steps. The code takes a user-provided topic as input, executes the workflow, and prints the final generated blog post. The state management is handled using a dictionary.\n",
      "\n",
      "üìù Step-by-Step Explanation:\n",
      " Excellent and thorough explanation! You've accurately broken down each part of the code, highlighting the key libraries, functionalities, and the workflow structure.\n",
      "\n",
      "Here are a few minor additions to make it even more comprehensive:\n",
      "\n",
      "*   **Error Handling/API Rate Limiting:** The code doesn't include any explicit error handling for API calls. In a real-world application, it's crucial to handle potential exceptions (e.g., network errors, API rate limits) to prevent the program from crashing and to provide informative error messages.  A more robust implementation would include `try...except` blocks around the API calls.\n",
      "\n",
      "*   **Prompt Engineering:** You could emphasize the importance of prompt engineering. The quality of the generated blog post heavily depends on the prompts used in the `generate_outline`, `expand_sections`, and `refine_content` functions.  Better prompts could lead to more coherent, informative, and engaging blog posts.\n",
      "\n",
      "*   **LangGraph Concepts:** While you explained the `StateGraph`, you could add a sentence or two explaining the benefit of using `LangGraph` in this context.  For example: \"LangGraph provides a way to structure complex AI workflows, allowing for easier management, modification, and debugging of multi-step processes compared to sequential function calls.\"\n",
      "\n",
      "*   **Model Choice (Gemini 2.0 Flash):** Briefly mention why `gemini-2.0-flash` might have been chosen (e.g., it's a faster and potentially cheaper model than other Gemini options, suitable for rapid prototyping).\n",
      "\n",
      "*   **Security:**  Reiterate the extreme importance of **not** hardcoding API keys. Explain that this is solely for demonstration purposes and a real application would use environment variables or a secrets management system.\n",
      "\n",
      "With these minor additions, your explanation is essentially perfect. You've demonstrated a strong understanding of the code's structure, functionality, and the underlying libraries.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# ‚úÖ Set API Key for Gemini\n",
    "api_key = \"AIzaSyBNX8KAXoSP2x3JejdKEX_Dit48YVJR1xg\"  # üî¥ Replace with your actual API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# ‚úÖ Initialize Gemini 2.0 Flash\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)\n",
    "\n",
    "# ‚úÖ Define State Schema for LangGraph\n",
    "class CodeExplainerState:\n",
    "    def __init__(self, code):\n",
    "        self.code = code\n",
    "        self.structure = None\n",
    "        self.explanation = None\n",
    "\n",
    "# ‚úÖ Step 1: Analyze Code Structure\n",
    "def analyze_code(state):\n",
    "    code_snippet = state.get(\"code\", \"No code provided\")\n",
    "\n",
    "    prompt = f\"Analyze the structure of this Python code and extract functions, classes, and key components:\\n\\n{code_snippet}\"\n",
    "    response = gemini.invoke(prompt)\n",
    "\n",
    "    if response and hasattr(response, \"content\") and response.content:\n",
    "        structure = response.content.strip()\n",
    "\n",
    "        # ‚úÖ Store code structure in state\n",
    "        return {\"code\": code_snippet, \"structure\": structure}\n",
    "\n",
    "    return {\"code\": code_snippet, \"structure\": \"Error: No response from AI\"}\n",
    "\n",
    "# ‚úÖ Step 2: Generate Step-by-Step Explanation\n",
    "def explain_code(state):\n",
    "    structure = state.get(\"structure\", \"No structure available\")\n",
    "\n",
    "    prompt = f\"Provide a step-by-step explanation of this Python code based on its structure:\\n{structure}\"\n",
    "    response = gemini.invoke(prompt)\n",
    "\n",
    "    if response and hasattr(response, \"content\") and response.content:\n",
    "        explanation = response.content.strip()\n",
    "\n",
    "        # ‚úÖ Store explanation in state\n",
    "        return {\"code\": state[\"code\"], \"structure\": structure, \"explanation\": explanation}\n",
    "\n",
    "    return {\"code\": state[\"code\"], \"structure\": structure, \"explanation\": \"Error: No response from AI\"}\n",
    "\n",
    "# ‚úÖ Build LangGraph Workflow\n",
    "workflow = StateGraph(dict)  # Using a dictionary-based state\n",
    "\n",
    "workflow.add_node(\"analyze_code\", analyze_code)\n",
    "workflow.add_node(\"explain_code\", explain_code)\n",
    "\n",
    "workflow.set_entry_point(\"analyze_code\")\n",
    "workflow.add_edge(\"analyze_code\", \"explain_code\")\n",
    "\n",
    "# ‚úÖ Compile the Workflow\n",
    "graph = workflow.compile()\n",
    "\n",
    "# ‚úÖ User Input and Execution\n",
    "user_code = input(\"Enter your Python code snippet: \")\n",
    "state = {\"code\": user_code}\n",
    "output = graph.invoke(state)\n",
    "\n",
    "# ‚úÖ Print Final Explanation (Only Once)\n",
    "print(\"\\nüìå Code Structure:\\n\", output.get(\"structure\", \"‚ùå Error: Missing code structure\"))\n",
    "print(\"\\nüìù Step-by-Step Explanation:\\n\", output.get(\"explanation\", \"‚ùå Error: Missing explanation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c8cd1-cfd5-48f2-a15f-5641cd886070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
